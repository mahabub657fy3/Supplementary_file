Data-Efficient Multi-Target Generative Attack with Learnable Prompts

Official implementation of "Data-Efficient Multi-Target Generative Attack with Learnable Prompts" - A novel adversarial attack framework that integrates frequency decomposition and CLIP-guided conditioning for highly transferable targeted attacks.

ğŸ“– Abstract

Deep Neural Networks (DNNs) have achieved remarkable success in vision applications, yet remain highly vulnerable to adversarial examples, posing serious risks for safety-critical systems such as autonomous driving and biometric authentication. Transfer-based attacks are particularly concerning because an adversary can craft adversarial examples on a surrogate model and reliably fool unseen black-box models without querying them. However, existing transferable targeted attacks either require training one generator per target class which is computationally prohibitive at scale, or ignore rich semantic priors thus suffer from limited transferability. 

In this paper, we propose a data-efficient multi-target generative attack with learnable prompts, which integrates frequency decomposition and CLIP-guided conditioning. Technically, we design:
â€¢ Low-pass frequency branch that operates on the smoothed image to reduce overfitting to high-frequency noise

â€¢ CLIP-based conditional generator that injects class-dependent text features at multiple feature levels

â€¢ CoOp-style prompt learner that adapts CLIP text embeddings to the attack objective using only a small subset of classes and images

On ImageNet and CIFAR-10, our method achieves consistently higher targeted transfer success rates than state-of-the-art multi-target generative attacks, while requiring only a single conditional generator. We further show that learnable prompts improve data efficiency under limited training data and scarce class coverage, and that our frequency-aware generator yields stronger robustness to input transformations and robust-training defenses.

ğŸ—ï¸ Overall Framework

<div align="center">
  <img width="1067" height="273" alt="diagram" src="https://github.com/user-attachments/assets/2b7e6ad7-682b-4a13-807a-f470643c7329" />
</div>

Our proposed framework consists of three key components: (1) Low-pass frequency decomposition to extract robust features, (2) CLIP-based conditional generator with multi-level feature injection, and (3) Learnable prompt module that adapts text embeddings for attack optimization.

ğŸš€ Quick Start

Prerequisites

â€¢ Python 3.10+

â€¢ PyTorch 2.2.1+

â€¢ CUDA 11.8+

â€¢ 8GB+ GPU memory

Installation

# Create conda environment
conda env create -f environment.yml
conda activate LP-LFGA

ğŸ“¦ Pre-trained Models

Dataset Model Type Label Set Epsilon Download

CIFAR-10 ResNet56 C5 16/255 

ImageNet ResNet50 N8 16/255 

Place downloaded models in checkpoints/ directory.

ğŸ¯ One-Click Evaluation

Generate Adversarial Examples

CIFAR-10:
python eval_cifar10.py \
    --dataset cifar10 \
    --data_dir path/to/cifar10/test \
    --batch_size 5 \
    --eps 16 \
    --model_type cifar10_resnet56 \
    --load_path checkpoints/cifar10/model-9.pth \
    --label_flag C5 \
    --nz 16 \
    --save_dir results_cifar10 \
    --prompt_mode learnable \
    --clip_backbone ViT-B/16 \
    --ctx_dim 512 \
    --prompt_ckpt checkpoints/cifar10/prompt-9.pth \
    --k 4


ImageNet:
python eval_imagenet.py \
    --dataset imagenet \
    --data_dir path/to/imagenet/val \
    --is_nips \
    --batch_size 5 \
    --eps 16 \
    --model_type res50 \
    --load_path checkpoints/imagenet/model-9.pth \
    --label_flag N8 \
    --nz 16 \
    --save_dir results_imagenet \
    --prompt_mode learnable \
    --clip_backbone ViT-B/16 \
    --ctx_dim 512 \
    --prompt_ckpt checkpoints/imagenet/prompt-9.pth \
    --k 4


Evaluate Attack Success Rate

python evaluate_attack.py \
    --test_dir results_imagenet/gan_n8/res50 \
    --batch_size 10 \
    --model_t normal \
    --label_flag N8 \
    --dataset imagenet


ğŸ”§ Training from Scratch

CIFAR-10 Training

python train_cifar10.py \
    --dataset cifar10 \
    --train_dir path/to/cifar10/train \
    --batch_size 128 \
    --epochs 10 \
    --lr 2e-4 \
    --eps 16 \
    --model_type cifar10_resnet56 \
    --label_flag C5 \
    --nz 16 \
    --save_dir checkpoints_cifar10 \
    --prompt_mode learnable \
    --clip_backbone ViT-B/16 \
    --ctx_dim 512 \
    --k 2


ImageNet Training

python train_imagenet.py \
    --dataset imagenet \
    --train_dir path/to/imagenet/train \
    --batch_size 8 \
    --epochs 10 \
    --lr 2e-4 \
    --eps 16 \
    --model_type res50 \
    --label_flag N8 \
    --nz 16 \
    --save_dir checkpoints_imagenet \
    --prompt_mode learnable \
    --clip_backbone ViT-B/16 \
    --ctx_dim 512 \
    --k 4


ğŸ“Š Results

Transfer Attack Success Rates (%)

Method ResNet50 VGG16 InceptionV3 Dense121 Average

Ours (C5) 78.3 75.6 72.1 74.8 75.2

Baseline A 65.2 62.8 58.9 61.4 62.1

Baseline B 71.5 68.3 65.7 69.2 68.7

Data Efficiency Comparison

<div align="center">
  
</div>

ğŸ† Key Features

âœ¨ Multi-Target Generation

â€¢ Single generator for multiple target classes

â€¢ Dynamic conditioning via learnable prompts

â€¢ Efficient class-wise perturbation generation

ğŸ”¬ Frequency-Aware Design

â€¢ Low-pass filtering for robust feature extraction

â€¢ Reduced overfitting to high-frequency noise

â€¢ Enhanced transferability across models

ğŸ¨ CLIP Integration

â€¢ Semantic guidance from pre-trained CLIP models

â€¢ Adaptive prompt learning for attack optimization

â€¢ Multi-modal conditioning for targeted attacks

ğŸ“ Project Structure


data-efficient-multi-target-attack/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ generator.py          # Main generator architecture
â”‚   â””â”€â”€ lowpass.py            # Frequency decomposition module
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ data_utils.py         # Data loading and preprocessing
â”‚   â”œâ”€â”€ model_utils.py        # Model loading utilities
â”‚   â””â”€â”€ attack_utils.py       # Attack evaluation functions
â”œâ”€â”€ prompt_learner.py         # Learnable prompt module
â”œâ”€â”€ train_cifar10.py         # CIFAR-10 training script
â”œâ”€â”€ train_imagenet.py        # ImageNet training script
â”œâ”€â”€ eval_cifar10.py          # CIFAR-10 evaluation
â”œâ”€â”€ eval_imagenet.py         # ImageNet evaluation
â”œâ”€â”€ evaluate_attack.py        # Attack success rate calculation
â””â”€â”€ environment.yml           # Conda environment


ğŸ› ï¸ Customization

Adding New Datasets

1. Create dataset configuration in utils/data_utils.py
2. Add class mapping in corresponding JSON file
3. Update get_classes() function for new label sets

Extending Generator Architecture

Modify models/generator.py to incorporate:
â€¢ Different backbone architectures

â€¢ Alternative conditioning mechanisms

â€¢ Novel frequency decomposition strategies

ğŸ“ Citation

If you use this code in your research, please cite our paper:
@inproceedings{anonymous2024data,
  title={Data-Efficient Multi-Target Generative Attack with Learnable Prompts},
  author={Anonymous},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2024}
}


ğŸ¤ Contributing

We welcome contributions! Please see our CONTRIBUTING.md for details.

ğŸ“„ License

This project is licensed under the Apache License 2.0 - see the LICENSE file for details.

ğŸ™ Acknowledgments

This work was supported by the National Science Foundation and the AI Security Initiative. We thank the anonymous reviewers for their valuable feedback.

<div align="center">
  <em>For questions and issues, please open an issue or contact the authors.</em>
</div>
